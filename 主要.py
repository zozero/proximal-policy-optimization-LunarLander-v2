import gym
import torch
import torch.nn as nn


class 记忆体:
    def __init__(self):
        self.行动_列表 = []
        self.状态_列表 = []
        self.概率质量_对数_列表 = []  # 概率质量函数的对数列表。
        self.奖励_列表 = []
        self.终止_列表 = []

    def 清空记忆体(self):
        del self.行动_列表[:]
        del self.状态_列表[:]
        del self.概率质量_对数_列表[:]
        del self.奖励_列表[:]
        del self.终止_列表[:]


class 艺术家与评论家(nn.Module):
    def __init__(self, 状态_维度, 动作_维度, 隐藏层_变量数):
        super(艺术家与评论家, self).__init__()
        ...


class 近端策略优化:
    def __init__(self, 状态_维度, 动作_维度, 隐藏层_变量数, 学习率, 贝塔值, 伽马值, 轮回次数, 夹子_间距):
        self.学习率 = 学习率
        self.贝塔值 = 贝塔值
        self.伽马值 = 伽马值
        self.轮回次数 = 轮回次数
        self.夹子_间距 = 夹子_间距

        self.策略 = 艺术家与评论家(状态_维度, 动作_维度, 隐藏层_变量数)


def 主要():
    游戏环境名 = "LunarLander-v2"
    游戏环境 = gym.make(游戏环境名, render_mode='human')
    状态_维度 = 游戏环境.observation_space.shape[0]
    动作_维度 = 4
    是否渲染 = True
    奖励_极值 = 230
    日志_间隔 = 20
    周期_最大值 = 50000
    """
        时间步长：
        许多物理方程将以这样的形式出现：“从现在开始 1 秒的速度 = 现在的速度 + 1 秒的加速度将产生的速度”（并且大多数不适合这种形式的方程可以以某种方式转换...... )
        “1 秒”部分是时间步长。定期改变世界的游戏会执行大量此类计算，并且要保持正常运行，需要使用一致的时间步长值评估所有这些物理方程。
        游戏经常使用接近每个渲染图形帧之间周期的时间步长，或图形帧之间的理想时间，例如 0.01667 秒，但这不是必需的。一些像 BeamNG.drive 这样的游戏会比游戏渲染更频繁地​​计算物理，以生成更平滑的物理近似。
        其他游戏将不那么频繁地计算物理并混合每个点之间渲染的位置。他们的方法将取决于每个游戏。
    """
    时间步长_最大值 = 300  # 时间的步长，这是一个单位
    隐藏层_变量数 = 64  # 隐藏层的变量数
    时间步长_更新_间隔 = 2000
    学习率 = 0.002
    贝塔值 = (0.9, 0.999)  # 用来计算梯度的平均数和平方的系数
    伽马值 = 0.99  # 折扣因子
    轮回次数 = 4  # 模型更新策略
    夹子_间距 = 0.2  # 夹子函数的范围
    随机_种子 = None

    if 随机_种子:
        torch.manual_seed(随机_种子)

    记忆体1 = 记忆体()

    近端策略优化1 = 近端策略优化(状态_维度, 动作_维度, 隐藏层_变量数, 学习率, 贝塔值, 伽马值, 轮回次数, 夹子_间距)
    pass


if __name__ == '__main__':
    主要()
